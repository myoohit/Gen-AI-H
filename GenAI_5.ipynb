{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2624724,"sourceType":"datasetVersion","datasetId":1595713}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:55:42.317767Z","iopub.execute_input":"2025-05-22T20:55:42.318734Z","iopub.status.idle":"2025-05-22T20:55:42.330128Z","shell.execute_reply.started":"2025-05-22T20:55:42.318696Z","shell.execute_reply":"2025-05-22T20:55:42.329108Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/google-word2vec/GoogleNews-vectors-negative300.bin\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#%pip install -q gensim nltk\n\nimport nltk\nfrom gensim.models import KeyedVectors\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('wordnet')\nnltk.download('punkt')\n\nmodel = KeyedVectors.load_word2vec_format(\"/kaggle/input/google-word2vec/GoogleNews-vectors-negative300.bin\", binary=True)\n\ndef get_similar(word, top_n=5):\n    try:\n        return [w[0] for w in model.most_similar(word, topn=top_n)]\n    except KeyError:\n        return []\n\ndef get_synonyms(word):\n    syns = set(lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas() if lemma.name().isalpha())\n    res = list(syns)[:5]\n    print(res)\n    return res\n\ndef generate_story(seed):\n    words = list(set(get_similar(seed, 3) + get_synonyms(seed)))\n    words += [seed] * (5 - len(words))\n    print(\"Words used in story:\", words[:5])\n    template = (\n        f\"Once upon a time, there was an ancient {seed}.\\n\"\n        f\"Legends spoke of the {words[0]}.\\n\"\n        f\"Under a {words[1]} sky, Alex set out on a journey.\\n\"\n        f\"Guided by {words[2]}, they found a hidden realm.\\n\"\n        f\"Inscriptions in {words[3]} revealed secrets of {words[4]}.\\n\"\n    )\n    return \" \".join(sent_tokenize(template))\n\nprint(generate_story(\"adventure\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T21:05:22.725494Z","iopub.execute_input":"2025-05-22T21:05:22.726077Z","iopub.status.idle":"2025-05-22T21:05:49.693795Z","shell.execute_reply.started":"2025-05-22T21:05:22.726049Z","shell.execute_reply":"2025-05-22T21:05:49.692654Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"['risk', 'escapade', 'hazard', 'gamble', 'adventure']\nWords used in story: ['risk', 'escapade', 'hazard', 'gamble', 'adventure']\nOnce upon a time, there was an ancient adventure. Legends spoke of the risk. Under a escapade sky, Alex set out on a journey. Guided by hazard, they found a hidden realm. Inscriptions in gamble revealed secrets of adventure.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}